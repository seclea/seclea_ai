{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1.Uploading the data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!ls \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install kaggle"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!kaggle datasets download -d buntyshah/auto-insurance-claims-data "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!unzip \\*.zip  && rm *.zip"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from seclea_ai import SecleaAI"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df=pd.read_csv('insurance_claims.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#2.Understand the dataset "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install seclea-ai"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seclea = SecleaAI(project_name=\"Insurance Claim\", plat_url=\"https://tristar-platform.seclea.com\")\n",
    "\n",
    "dataset_metadata = {\"index\": None, \"outcome_name\": \"fraud_reported\", \"continuous_features\": [\"total_claim_amount\",'policy_annual_premium','capital-gains','capital-loss','injury_claim','property_claim','vehicle_claim','incident_hour_of_the_day',]}\n",
    "\n",
    "# seclea.upload_dataset(dataset=\"insurance_claims.csv\", dataset_name=\"Fraud_Insurance_Claim\", metadata=dataset_metadata)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Checking for the missing values\n",
    "\n",
    "df.isnull().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.head(100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def encode_nans(df):\n",
    "    #dealing with special character\n",
    "    df['collision_type'] = df['collision_type'].replace('?', np.NaN)\n",
    "    df['property_damage'] = df['property_damage'].replace('?', np.NaN)\n",
    "    df['police_report_available'] = df['police_report_available'].replace('?', \"NO\") # default to no police report present if previously ?\n",
    "    return df\n",
    "\n",
    "df = encode_nans(df)\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# checking for the null Values \n",
    "df.isnull().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Checking for the missing values \n",
    "df.isnull().any().any()\n",
    "# seclea.upload_dataset(dataset=\"insurance_claims.csv\", dataset_name=\"Fraud_Insurance_Claim\", metadata=dataset_metadata)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##Check the correlation\n",
    "df.corr()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## drop the most correlated data \n",
    "threshold = 0.97\n",
    "def drop_correlated(data, thresh):\n",
    "    # calculate correlations\n",
    "    corr_matrix = data.corr().abs()\n",
    "    # get the upper part of correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "    # columns with correlation above threshold\n",
    "    redundant = [column for column in upper.columns if any(upper[column] >= thresh)]\n",
    "    print(f'Columns to drop with correlation > {threshold}: {redundant}')\n",
    "    data.drop(columns=redundant, inplace=True)\n",
    "    return data\n",
    "\n",
    "df = drop_correlated(df, threshold)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.Data preprocessing "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#droping the column with more than 90% of missing value\n",
    "def drop_nulls(df, threshold):\n",
    "    cols = [x for x in df.columns if df[x].isnull().sum() / df.shape[0] > threshold]\n",
    "    return df.drop(columns=cols)\n",
    "\n",
    "df = drop_nulls(df, threshold=0.9)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Catagorical Data \n",
    "cat_cols=df.select_dtypes(include=['object']).columns.tolist()\n",
    "cat_cols\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def encode_categorical(df,cat_cols):\n",
    "  from sklearn.preprocessing import LabelEncoder\n",
    "  for col in cat_cols:\n",
    "    if col in df.columns:\n",
    "        le = LabelEncoder()\n",
    "        le.fit(list(df[col].astype(str).values))\n",
    "        df[col] = le.transform(list(df[col].astype(str).values))\n",
    "  return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cat_cols=df.select_dtypes(include=['object']).columns.tolist()\n",
    "df = encode_categorical(df, cat_cols)\n",
    "df.fillna({\"collision_type\": -1, \"property_damage\": -1})\n",
    "df.info()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# store cleared dataset\n",
    "df.to_csv('Insurance_cleaned.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv('Insurance_cleaned.csv')\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define transformation functions\n",
    "transformations = [encode_nans, (drop_correlated, [0.97]), (drop_nulls, [0.9]), (encode_categorical, {\"cat_cols\": cat_cols})]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop('fraud_reported', axis=1)\n",
    "y = df.fraud_reported\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f'''% Positive class in Train = {np.round(y_train.value_counts(normalize=True)[1] * 100, 2)}\n",
    "% Positive class in Test  = {np.round(y_test.value_counts(normalize=True)[1] * 100, 2)}''')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### testing without SMOTE\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_test)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4.Balancing the dataset with SMOTE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE \n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "\n",
    "# oversample train set (don't oversample the test ever)\n",
    "X_sm, y_sm = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f'''Shape of X before SMOTE: {X_train.shape}\n",
    "Shape of X after SMOTE: {X_sm.shape}''')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Normalization\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# fit the scaler (train samples only)\n",
    "scaler = StandardScaler().fit(X_sm)\n",
    "\n",
    "# transform (scale)\n",
    "X_sm_scaled = scaler.transform(X_sm)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "#train\n",
    "knn_scaled = KNeighborsClassifier()\n",
    "knn_scaled.fit(X_sm_scaled, y_sm)\n",
    "#testing\n",
    "preds_scaled = knn_scaled.predict(X_test_scaled)\n",
    "# Evaluate accuracy\n",
    "print(accuracy_score(y_test, preds_scaled))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### testing with confusion matrix with Smote\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "\n",
    "# Train\n",
    "model =  RandomForestClassifier(random_state=42)\n",
    "model.fit(X_sm, y_sm)\n",
    "preds_forest = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(f'Accuracy = {accuracy_score(y_test, preds_forest):.2f}\\nRecall = {recall_score(y_test, preds_forest):.2f}\\n')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5.Hyper-parameter Tuning with various models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    " ### Modeling\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "classifiers = {\n",
    "    \"LogisticRegression\": LogisticRegression(),\n",
    "    \"GradientBoostingClassifier\": GradientBoostingClassifier(),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier()\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## score from various models \n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "for key, classifier in classifiers.items():\n",
    "    classifier.fit(X_sm, y_sm)\n",
    "    training_score = cross_val_score(classifier, X_sm, y_sm, cv=5)\n",
    "    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    " ##Use GridSearchCV to find the best parameters.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# Logistic Regression \n",
    "log_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "\n",
    "grid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params)\n",
    "grid_log_reg.fit(X_sm, y_sm)\n",
    "# We automatically get the logistic regression with the best parameters.\n",
    "log_reg = grid_log_reg.best_estimator_\n",
    "\n",
    "# DecisionTree Classifier\n",
    "tree_params = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,4,1)), \n",
    "              \"min_samples_leaf\": list(range(5,7,1))}\n",
    "grid_tree = GridSearchCV(DecisionTreeClassifier(), tree_params)\n",
    "grid_tree.fit(X_sm_scaled, y_sm)\n",
    "\n",
    "# tree best estimator\n",
    "tree_clf = grid_tree.best_estimator_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Overfitting Case\n",
    "\n",
    "log_reg_score = cross_val_score(log_reg, X_sm_scaled, y_sm, cv=5)\n",
    "print('Logistic Regression Cross Validation Score: ', round(log_reg_score.mean() * 100, 2).astype(str) + '%')\n",
    "\n",
    "tree_score = cross_val_score(tree_clf, X_sm_scaled, y_sm, cv=5)\n",
    "print('DecisionTree Classifier Cross Validation Score', round(tree_score.mean() * 100, 2).astype(str) + '%')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Insurance fraud dataset .ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}