{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Getting Started with Seclea!",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"https://github.com/seclea/seclea_ai/raw/dev/docs/media/logos/logo-light.png\" width=\"400\" alt=\"Seclea\" />"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Getting Started\n",
    "\n",
    "We will run through a sample project showing how to use Seclea's tools to record your data science work\n",
    "and explore the results in the Seclea Platform.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set up the Project\n",
    "\n",
    "Head to [platform.seclea.com](https://platform.seclea.com) and log in.\n",
    "\n",
    "Create a new project and give it a name and description.\n",
    "\n",
    "![](https://github.com/seclea/seclea_ai/raw/dev/docs/media/notebooks/getting_started/create-new-project.png)\n",
    "![](https://github.com/seclea/seclea_ai/raw/dev/docs/media/notebooks/getting_started/create-project-name-description.png)\n",
    "\n",
    "- Go to project settings\n",
    "- Select Compliance, Risk and Performance Templates for this project.\n",
    "\n",
    "These are optional but are needed to take advantage of Checks. If in doubt leave these empty for now and come back."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Integrate with seclea-ai\n",
    "\n",
    "You can get the seclea-ai package from either pip or conda-forge - whichever you prefer!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install seclea_ai\n",
    "# !conda install seclea_ai"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "When you initialise the SecleaAI object you will be prompted to login if you haven't already done so.\n",
    "Use the same Project Name you used earlier and the Organization name provided with your credentials.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from seclea_ai import SecleaAI\n",
    "\n",
    "# NOTE - use the organization name provided to you when issued credentials.\n",
    "seclea = SecleaAI(project_name=\"Car Insurance Fraud Detection\", organization='Onespan', platform_url=\"http://localhost:8000\", auth_url=\"http://localhost:8010\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## üóÑ Handle the Data\n",
    "\n",
    "Download the [data](https://raw.githubusercontent.com/seclea/seclea_ai/dev/docs/examples/insurance_claims.csv) for\n",
    "this tutorial if you are working on this in Colab or without reference to the repo - this is an Insurance Claims dataset with a variety of features and 1000 samples.\n",
    "\n",
    "Now we can upload the initial data to the Seclea Platform. \n",
    "\n",
    "This should include whatever information we know about the dataset at this point as metadata. \n",
    "There are only two keys to add in metadata for now - outcome_name and continuous_features.\n",
    "\n",
    "You can leave out outcome_name if you haven't decided what you will be predicting yet, but you should\n",
    "know or be able to find out the continuous features at this point.\n",
    "\n",
    "You can also update these when uploading datasets\n",
    "during/after pre-processing. \n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# load the data \n",
    "data = pd.read_csv('insurance_claims.csv', index_col=\"policy_number\")\n",
    "\n",
    "# define the metadata for the dataset.\n",
    "dataset_metadata = {\"outcome_name\": \"fraud_reported\",\n",
    "                    \"favourable_outcome\": \"N\",\n",
    "                    \"unfavourable_outcome\": \"Y\",\n",
    "                    \"continuous_features\": [\n",
    "                                            \"total_claim_amount\",\n",
    "                                            'policy_annual_premium',\n",
    "                                            'capital-gains',\n",
    "                                            'capital-loss',\n",
    "                                            'injury_claim',\n",
    "                                            'property_claim',\n",
    "                                            'vehicle_claim',\n",
    "                                            'incident_hour_of_the_day',\n",
    "                                            ]}\n",
    "\n",
    "\n",
    "# ‚¨ÜÔ∏è upload the dataset - pick a meaningful name here, you'll be seeing it a lot on the platform!\n",
    "seclea.upload_dataset(dataset=data, dataset_name=\"Auto Insurance Fraud\", metadata=dataset_metadata)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### üîç Evaluating the Dataset\n",
    "\n",
    "Head back to the platform, so we can take a look at our Dataset\n",
    "\n",
    "Navigate to the Datasets section - under Prepare tab. See the preview and use the format check/PII check.\n",
    "\n",
    "PII  and Format Check\n",
    "\n",
    "Bias Check\n",
    "\n",
    "Include screen shots."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## üîÄ Transformations\n",
    "\n",
    "When using Seclea to record your Data Science work you will have to take care with how you\n",
    "deal with transformations of the data.\n",
    "\n",
    "We require that all transformations are encapsulated in a function, that takes the data and returns the\n",
    "transformed data. There are a few things to be aware of so please see the [docs](https://docs.seclea.com) for more."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Create a copy to isolate the original dataset\n",
    "df1 = data.copy(deep=True)\n",
    "\n",
    "def encode_nans(df):\n",
    "    # convert the special characters to nans\n",
    "    return df.replace('?', np.NaN)\n",
    "\n",
    "df2 = encode_nans(df1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## üßΩ Data Cleaning\n",
    "\n",
    "We will carry out some pre-processing and generate a few different datasets so that we\n",
    "can see on the platform how to track these.This also means we can train our models on some\n",
    "different data and see how that affects performance."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "## Drop the the column which are more than some proportion NaN values\n",
    "def drop_nulls(df, threshold):\n",
    "    cols = [x for x in df.columns if df[x].isnull().sum() / df.shape[0] > threshold]\n",
    "    return df.drop(columns=cols)\n",
    "\n",
    "# We choose 95% as our threshold\n",
    "null_thresh = 0.95\n",
    "df3 = drop_nulls(df2, threshold=null_thresh)\n",
    "\n",
    "def drop_correlated(data, thresh):\n",
    "    import numpy as np\n",
    "\n",
    "    # calculate correlations\n",
    "    corr_matrix = data.corr().abs()\n",
    "    # get the upper part of correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "    # columns with correlation above threshold\n",
    "    redundant = [column for column in upper.columns if any(upper[column] >= thresh)]\n",
    "    print(f\"Columns to drop with correlation > {thresh}: {redundant}\")\n",
    "    new_data = data.drop(columns=redundant)\n",
    "    return new_data\n",
    "\n",
    "# drop columns that are too closely correlated\n",
    "correlation_threshold = 0.95\n",
    "df4 = drop_correlated(df3, correlation_threshold)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ‚¨ÜÔ∏è Upload Intermediate Dataset\n",
    "\n",
    "Before getting to balancing the datasets we will upload them to the Seclea Platform.\n",
    "\n",
    "- We define the metadata for the dataset - if there have been any changes since the original dataset we need to put that here, otherwise we can reuse the original metadata. In this case we have dropped some of the continuous feature columns so we will need to redefine\n",
    "\n",
    "- We define the transformations that took place between the last state we uploaded and this dataset. This is a list of functions and arguments. See docs.seclea.com for more details of the correct formatting.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from seclea_ai.transformations import DatasetTransformation\n",
    "\n",
    "# define the updates to the metadata - empty as there are no changes\n",
    "processed_metadata = {}\n",
    "\n",
    "# üîÄ define the transformations - note the arguments\n",
    "cleaning_transformations = [\n",
    "            DatasetTransformation(encode_nans, data_kwargs={\"df\": df1}, kwargs={}, outputs=[\"df\"]),\n",
    "            DatasetTransformation(\n",
    "                drop_nulls, data_kwargs={\"df\": \"inherit\"}, kwargs={\"threshold\": null_thresh}, outputs=[\"data\"]\n",
    "            ),\n",
    "            DatasetTransformation(\n",
    "                drop_correlated, data_kwargs={\"data\": \"inherit\"}, kwargs={\"thresh\": correlation_threshold}, outputs=[\"df\"]\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "# ‚¨ÜÔ∏è upload the cleaned datasets\n",
    "seclea.upload_dataset(dataset=df4,\n",
    "                      dataset_name=\"Auto Insurance Fraud - Cleaned\",\n",
    "                      metadata=processed_metadata,\n",
    "                      transformations=cleaning_transformations)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X before SMOTE: (800, 35)\n",
      "    Shape of X after SMOTE: (1204, 35)\n",
      "Shape of y before SMOTE: (800,)\n",
      "    Shape of y after SMOTE: (1204,)\n",
      "Shape of X before SMOTE: (800, 35)\n",
      "    Shape of X after SMOTE: (1204, 35)\n",
      "Shape of y before SMOTE: (800,)\n",
      "    Shape of y after SMOTE: (1204,)\n",
      "Shape of X before SMOTE: (800, 35)\n",
      "    Shape of X after SMOTE: (1204, 35)\n",
      "Shape of y before SMOTE: (800,)\n",
      "    Shape of y after SMOTE: (1204,)\n",
      "Shape of X before SMOTE: (800, 35)\n",
      "    Shape of X after SMOTE: (1204, 35)\n",
      "Shape of y before SMOTE: (800,)\n",
      "    Shape of y after SMOTE: (1204,)\n"
     ]
    }
   ],
   "source": [
    "def fill_nan_const(df, val):\n",
    "    \"\"\"Fill NaN values in the dataframe with a constant value\"\"\"\n",
    "    return df.replace(['None', np.nan], val)\n",
    "\n",
    "\n",
    "# Fill nans in 1st dataset with -1\n",
    "const_val = -1\n",
    "df_const = fill_nan_const(df4, const_val)\n",
    "\n",
    "def fill_nan_mode(df, columns):\n",
    "    \"\"\"\n",
    "    Fills nans in specified columns with the mode of that column\n",
    "    Note that we want to make sure to not modify the dataset we passed in but to\n",
    "    return a new copy.\n",
    "    We do that by making a copy and specifying deep=True.\n",
    "    \"\"\"\n",
    "    new_df = df.copy(deep=True)\n",
    "    for col in df.columns:\n",
    "        if col in columns:\n",
    "            new_df[col] = df[col].fillna(df[col].mode()[0])\n",
    "    return new_df\n",
    "\n",
    "\n",
    "nan_cols = ['collision_type','property_damage', 'police_report_available']\n",
    "df_mode = fill_nan_mode(df4, nan_cols)\n",
    "\n",
    "\n",
    "# find columns with categorical data for both dataset\n",
    "cat_cols = df_const.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "def encode_categorical(df, cat_cols): \n",
    "  from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "  new_df = df.copy(deep=True)\n",
    "  for col in cat_cols:\n",
    "    if col in df.columns:\n",
    "        le = LabelEncoder()\n",
    "        le.fit(list(df[col].astype(str).values))\n",
    "        new_df[col] = le.transform(list(df[col].astype(str).values))\n",
    "  return new_df\n",
    "\n",
    "df_const = encode_categorical(df_const, cat_cols)\n",
    "df_mode = encode_categorical(df_mode, cat_cols)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# üîÄ define the transformations - for the constant fill dataset\n",
    "const_processed_transformations = [\n",
    "    DatasetTransformation(fill_nan_const, data_kwargs={\"df\": df4}, kwargs={\"val\": const_val}, outputs=[\"df\"]),\n",
    "    DatasetTransformation(encode_categorical, data_kwargs={\"df\": \"inherit\"}, kwargs={\"cat_cols\":cat_cols}, outputs=[\"df\"]),\n",
    "]\n",
    "\n",
    "# ‚¨ÜÔ∏è upload the constant fill dataset\n",
    "seclea.upload_dataset(dataset=df_const, \n",
    "                      dataset_name=\"Auto Insurance Fraud - Const Fill\", \n",
    "                      metadata=processed_metadata,\n",
    "                      transformations=const_processed_transformations)\n",
    "\n",
    "# üîÄ define the transformations - for the mode fill dataset\n",
    "mode_processed_transformations = [\n",
    "    DatasetTransformation(fill_nan_mode, data_kwargs={\"df\": df4}, kwargs={\"columns\": nan_cols}, outputs=[\"df\"]),\n",
    "    DatasetTransformation(encode_categorical, data_kwargs={\"df\": \"inherit\"}, kwargs={\"cat_cols\": cat_cols}, outputs=[\"df\"]),\n",
    "]\n",
    "\n",
    "# ‚¨ÜÔ∏è upload the mode fill dataset\n",
    "seclea.upload_dataset(dataset=df_mode,\n",
    "                      dataset_name=\"Auto Insurance Fraud - Mode Fill\",\n",
    "                      metadata=processed_metadata,\n",
    "                      transformations=mode_processed_transformations)\n",
    "\n",
    "def get_samples_labels(df, output_col):\n",
    "    X = df.drop(output_col, axis=1)\n",
    "    y = df[output_col]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# split the datasets into samples and labels ready for modelling.\n",
    "X_const, y_const = get_samples_labels(df_const, \"fraud_reported\")\n",
    "X_mode, y_mode = get_samples_labels(df_mode, \"fraud_reported\")\n",
    "\n",
    "def get_test_train_splits(X, y, test_size, random_state):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    return train_test_split(\n",
    "        X, y, test_size=test_size, stratify=y, random_state=random_state\n",
    "    )\n",
    "    # returns X_train, X_test, y_train, y_test\n",
    "\n",
    "# split into test and train sets\n",
    "X_train_const, X_test_const, y_train_const, y_test_const = get_test_train_splits(X_const, y_const, test_size=0.2, random_state=42)\n",
    "X_train_mode, X_test_mode, y_train_mode, y_test_mode = get_test_train_splits(X_mode, y_mode, test_size=0.2, random_state=42)\n",
    "\n",
    "# üîÄ define the transformations - for the constant fill training set\n",
    "const_train_transformations = [\n",
    "    DatasetTransformation(\n",
    "            get_test_train_splits,\n",
    "            data_kwargs={\"X\": X_const, \"y\": y_const},\n",
    "            kwargs={\"test_size\": 0.2, \"random_state\": 42},\n",
    "            outputs=[\"X_train_const\", None, \"y_train_const\", None],\n",
    "            split=\"train\",\n",
    "            ),\n",
    "]\n",
    "\n",
    "# ‚¨ÜÔ∏è upload the const fill training set\n",
    "seclea.upload_dataset_split(\n",
    "                        X=X_train_const,\n",
    "                        y=y_train_const,\n",
    "                        dataset_name=\"Auto Insurance Fraud - Const Fill - Train\",\n",
    "                        metadata=processed_metadata,\n",
    "                        transformations=const_train_transformations\n",
    ")\n",
    "\n",
    "# üîÄ define the transformations - for the constant fill test set\n",
    "const_test_transformations = [\n",
    "    DatasetTransformation(\n",
    "            get_test_train_splits,\n",
    "            data_kwargs={\"X\": X_const, \"y\": y_const},\n",
    "            kwargs={\"test_size\": 0.2, \"random_state\": 42},\n",
    "            outputs=[None, \"X_test_const\", None, \"y_test_const\"],\n",
    "            split=\"test\"\n",
    "            ),\n",
    "]\n",
    "\n",
    "# ‚¨ÜÔ∏è upload the const fill test set\n",
    "seclea.upload_dataset_split(X=X_test_const,\n",
    "                      y=y_test_const,\n",
    "                      dataset_name=\"Auto Insurance Fraud - Const Fill - Test\",\n",
    "                      metadata=processed_metadata,\n",
    "                      transformations=const_test_transformations)\n",
    "\n",
    "# üîÄ define the transformations - for the mode fill training set\n",
    "mode_train_transformations = [\n",
    "    DatasetTransformation(\n",
    "            get_test_train_splits,\n",
    "            data_kwargs={\"X\": X_mode, \"y\": y_mode},\n",
    "            kwargs={\"test_size\": 0.2, \"random_state\": 42},\n",
    "            outputs=[\"X_train_mode\", None, \"y_train_mode\", None],\n",
    "            split=\"train\",\n",
    "            ),\n",
    "]\n",
    "\n",
    "# ‚¨ÜÔ∏è upload the mode fill train set\n",
    "seclea.upload_dataset_split(X=X_train_mode,\n",
    "                      y=y_train_mode,\n",
    "                      dataset_name=\"Auto Insurance Fraud - Mode Fill - Train\",\n",
    "                      metadata=processed_metadata,\n",
    "                      transformations=mode_train_transformations)\n",
    "\n",
    "# üîÄ define the transformations - for the mode fill test set\n",
    "mode_test_transformations = [\n",
    "    DatasetTransformation(\n",
    "            get_test_train_splits,\n",
    "            data_kwargs={\"X\": X_mode, \"y\": y_mode},\n",
    "            kwargs={\"test_size\": 0.2, \"random_state\": 42},\n",
    "            outputs=[None, \"X_test_mode\", None, \"y_test_mode\"],\n",
    "            split=\"test\",\n",
    "            ),\n",
    "]\n",
    "\n",
    "# ‚¨ÜÔ∏è upload the mode fill test set\n",
    "seclea.upload_dataset_split(X=X_test_mode,\n",
    "                      y=y_test_mode,\n",
    "                      dataset_name=\"Auto Insurance Fraud - Mode Fill - Test\",\n",
    "                      metadata=processed_metadata,\n",
    "                      transformations=mode_test_transformations)\n",
    "\n",
    "\n",
    "\n",
    "def smote_balance(X, y, random_state):\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "\n",
    "    sm = SMOTE(random_state=random_state)\n",
    "\n",
    "    X_sm, y_sm = sm.fit_resample(X, y)\n",
    "\n",
    "    print(\n",
    "        f\"\"\"Shape of X before SMOTE: {X.shape}\n",
    "    Shape of X after SMOTE: {X_sm.shape}\"\"\"\n",
    "    )\n",
    "    print(\n",
    "        f\"\"\"Shape of y before SMOTE: {y.shape}\n",
    "    Shape of y after SMOTE: {y_sm.shape}\"\"\"\n",
    "    )\n",
    "    return X_sm, y_sm\n",
    "    # returns X, y\n",
    "\n",
    "# balance the training sets - creating new training sets for comparison\n",
    "X_train_const_smote, y_train_const_smote = smote_balance(X_train_const, y_train_const, random_state=42)\n",
    "X_train_mode_smote, y_train_mode_smote = smote_balance(X_train_mode, y_train_mode, random_state=42)\n",
    "\n",
    "# üîÄ define the transformations - for the constant fill balanced train set\n",
    "const_smote_transformations = [\n",
    "    DatasetTransformation(\n",
    "            smote_balance,\n",
    "            data_kwargs={\"X\": X_train_const, \"y\": y_train_const},\n",
    "            kwargs={\"random_state\": 42},\n",
    "            outputs=[\"X\", \"y\"]\n",
    "            ),\n",
    "]\n",
    "\n",
    "# ‚¨ÜÔ∏è upload the constant fill balanced train set\n",
    "seclea.upload_dataset_split(X=X_train_const_smote,\n",
    "                      y=y_train_const_smote,\n",
    "                      dataset_name=\"Auto Insurance Fraud - Const Fill - Smote Train\",\n",
    "                      metadata=processed_metadata,\n",
    "                      transformations=const_smote_transformations)\n",
    "\n",
    "# üîÄ define the transformations - for the mode fill balanced train set\n",
    "mode_smote_transformations = [\n",
    "    DatasetTransformation(\n",
    "            smote_balance,\n",
    "            data_kwargs={\"X\": X_train_mode, \"y\": y_train_mode},\n",
    "            kwargs={\"random_state\": 42},\n",
    "            outputs=[\"X\", \"y\"]\n",
    "            ),\n",
    "]\n",
    "\n",
    "# ‚¨ÜÔ∏è upload the mode fill balanced train set\n",
    "seclea.upload_dataset_split(X=X_train_mode_smote,\n",
    "                      y=y_train_mode_smote,\n",
    "                      dataset_name=\"Auto Insurance Fraud - Mode Fill - Smote Train\",\n",
    "                      metadata=processed_metadata,\n",
    "                      transformations=mode_smote_transformations)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### üîç Evaluating the Transformations\n",
    "\n",
    "Now head to platform.seclea.com again to take another look at the Datasets section. You will see that there is a lot more to look at this time.\n",
    "\n",
    "You can see here how the transformations are used to show you the history of the data and how it arrived in its final state."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# üõ†Ô∏è Modeling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we get started with the modelling. We will run the same models over each of our datasets to explore how the different processing of the data has affected our results.\n",
    "\n",
    "We will use three models from sklearn for this, DecisionTree, RandomForest and GradientBoosting Classifers. \n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## üìà Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "classifiers = {\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
    "    \"GradientBoostingClassifier\": GradientBoostingClassifier()\n",
    "}\n",
    "\n",
    "datasets = [\n",
    "    (\"Const Fill\", (X_train_const, X_test_const, y_train_const, y_test_const)),\n",
    "    (\"Mode Fill\", (X_train_mode, X_test_mode, y_train_mode, y_test_mode)),\n",
    "    (\"Const Fill Smote\", (X_train_const_smote, X_test_const, y_train_const_smote, y_test_const)),\n",
    "    (\"Mode Fill Smote\", (X_train_mode_smote, X_test_mode, y_train_mode_smote, y_test_mode))\n",
    "    ]\n",
    "\n",
    "for name, (X_train, X_test, y_train, y_test) in datasets:\n",
    "\n",
    "    for key, classifier in classifiers.items():\n",
    "        # cross validate to get an idea of generalisation.\n",
    "        training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n",
    "\n",
    "        # train on the full training set\n",
    "        classifier.fit(X_train, y_train)\n",
    "\n",
    "        # ‚¨ÜÔ∏è upload the fully trained model\n",
    "        seclea.upload_training_run_split(model=classifier, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test)\n",
    "\n",
    "        # test accuracy\n",
    "        y_preds = classifier.predict(X_test)\n",
    "        test_score = accuracy_score(y_test, y_preds)\n",
    "        print(f\"Classifier: {classifier.__class__.__name__} has a training score of {round(training_score.mean(), 3) * 100}% accuracy score on {name}\")\n",
    "        print(f\"Classifier: {classifier.__class__.__name__} has a test score of {round(test_score, 3) * 100}% accuracy score on {name}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## üîç Analysis\n",
    "\n",
    "Head back to [platform.seclea.com](https://platform.seclea.com) and we can analyse our Models\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ]
}