{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Getting Started with Seclea!",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"https://github.com/seclea/seclea_ai/raw/dev/docs/media/logos/logo-light.png\" width=\"400\" alt=\"Seclea\" />"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Getting Started\n",
    "\n",
    "We will run through the basic process of using Seclea to record your data science work\n",
    "and explore the results in the Seclea Platform.\n",
    "\n",
    "For non data-scientists you will want to pay most attention to the Seclea Platform sections."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setting up\n",
    "\n",
    "Head to [platform.seclea.com](https://platform.seclea.com) and log in.\n",
    "\n",
    "Create a new project and give it a name and description.\n",
    "\n",
    "<img src=\"https://github.com/seclea/seclea_ai/media/notebooks/getting_started/create-new-project.png\" width=300/>\n",
    "<img src=\"https://github.com/seclea/seclea_ai/media/notebooks/getting_started/create-project-name-description.png\" width=300/>\n",
    "\n",
    "Go to project settings\n",
    "\n",
    "<img src=\"https://github.com/seclea/seclea_ai/media/notebooks/getting_started/project-settings.png\" width=300/>\n",
    "\n",
    "Select Compliance, Risk and Performance Templates for this project.\n",
    "These are optional but are needed to take advantage of Checks. If in doubt leave these empty for now and come back.\n",
    "\n",
    "TODO fill in images --- here ---"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Data\n",
    "\n",
    "[Download](https://raw.githubusercontent.com/mwitiderrick/insurancedata/master/insurance_claims.csv) the data for this tutorial"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install seclea-ai"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can upload the initial data to the Seclea Platform. This should include whatever information we know about the dataset at this point as metadata.\n",
    "\n",
    "There are only two keys to add in metadata for now - outcome_name and continuous_features.\n",
    "\n",
    "Here you will also have to log in to the Platform using the credentials given to you."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from seclea_ai import SecleaAI\n",
    "\n",
    "# load the data \n",
    "data = pd.read_csv('insurance_claims.csv', index=\"policy_number\")\n",
    "\n",
    "# upload the data in its initial state to the Seclea Platform\n",
    "# NOTE - use the organization name provided to you when issued credentials.\n",
    "seclea = SecleaAI(project_name=\"My Project\", organization='My Org')\n",
    "\n",
    "dataset_metadata = {\"outcome_name\": \"fraud_reported\", \n",
    "                    \"continuous_features\": [\n",
    "                                            \"total_claim_amount\",\n",
    "                                            'policy_annual_premium',\n",
    "                                            'capital-gains',\n",
    "                                            'capital-loss',\n",
    "                                            'injury_claim',\n",
    "                                            'property_claim',\n",
    "                                            'vehicle_claim',\n",
    "                                            'incident_hour_of_the_day',\n",
    "                                            ]}\n",
    "\n",
    "seclea.upload_dataset(dataset=data, dataset_name=\"Auto Insurance Fraud\", metadata=dataset_metadata)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Seclea Platform - Datasets\n",
    "\n",
    "Heading back to the platform we can take a look at our Dataset\n",
    "\n",
    "Navigate to the Datasets section - under Prepare tab. See the preview and use the format check/PII check.\n",
    "\n",
    "PII  and Format Check\n",
    "\n",
    "Bias Check\n",
    "\n",
    "Include screen shots."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transformations\n",
    "\n",
    "There is one important requirement when using Seclea to record your Data Science work, that is how\n",
    "to deal with transformations of the data.\n",
    "\n",
    "We require that all transformations are encapsulated in a function, that takes the data and returns the\n",
    "transformed data. There are a few things to be aware of so please see the [docs](https://docs.seclea.com) for more."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Creating a copy to isolate the original dataset\n",
    "df1 = data.copy(deep=True)\n",
    "\n",
    "def encode_nans(df):\n",
    "    # converting the special characters to nans\n",
    "    return df.replace('?', np.NaN)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df1 = encode_nans(df1)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data preprocessing/Feature Engineering\n",
    "\n",
    "We will carry out some preprocessing and generate a few different datasets so that we\n",
    "can see on the platform how we track these and also so we can train our models on some\n",
    "different data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Drop the the column with certain proportion NaN value \n",
    "def drop_nulls(df, threshold):\n",
    "    cols = [x for x in df.columns if df[x].isnull().sum() / df.shape[0] > threshold]\n",
    "    return df.drop(columns=cols)\n",
    "\n",
    "# We choose 95% as our threshold\n",
    "null_thresh = 0.95\n",
    "df1 = drop_nulls(df1, threshold=null_thresh)\n",
    "\n",
    "def fill_nan_const(df, val):\n",
    "    \"\"\"Fill NaN values in the dataframe with a constant value\"\"\"\n",
    "    return df.replace(['None', np.nan], val)\n",
    "\n",
    "\n",
    "# Fill nans in 1st dataset with -1\n",
    "const_val = -1\n",
    "df_const = fill_nan_const(df1, const_val)\n",
    "\n",
    "def fill_nan_mode(df, columns):\n",
    "    \"\"\"\n",
    "    Fills nans in specified columns with the mode of that column\n",
    "    Note that we want to make sure to not modify the dataset we passed in but to\n",
    "    return a new copy.\n",
    "    We do that by making a copy and specifying deep=True.\n",
    "    \"\"\"\n",
    "    new_df = df.copy(deep=True)\n",
    "    for col in df.columns:\n",
    "        if col in columns:\n",
    "            new_df[col] = df[col].fillna(df[col].mode()[0])\n",
    "    return new_df\n",
    "\n",
    "\n",
    "nan_cols = ['collision_type','property_damage','police_report_available']\n",
    "df_mode = fill_nan_mode(df1, nan_cols)\n",
    "\n",
    "\n",
    "def drop_correlated(data, thresh):\n",
    "    import numpy as np\n",
    "\n",
    "    # calculate correlations\n",
    "    corr_matrix = data.corr().abs()\n",
    "    # get the upper part of correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "    # columns with correlation above threshold\n",
    "    redundant = [column for column in upper.columns if any(upper[column] >= thresh)]\n",
    "    print(f\"Columns to drop with correlation > {thresh}: {redundant}\")\n",
    "    new_data = data.drop(columns=redundant)\n",
    "    return new_data\n",
    "\n",
    "correlation_threshold = 0.9\n",
    "\n",
    "df_const = drop_correlated(df_const, correlation_threshold)\n",
    "df_mode = drop_correlated(df_mode, correlation_threshold)\n",
    "\n",
    "# find columns with categorical data for both dataset\n",
    "cat_cols = df_const.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "def encode_categorical(df, cat_cols): \n",
    "  from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "  new_df = df.copy(deep=True)\n",
    "  for col in cat_cols:\n",
    "    if col in df.columns:\n",
    "        le = LabelEncoder()\n",
    "        le.fit(list(df[col].astype(str).values))\n",
    "        new_df[col] = le.transform(list(df[col].astype(str).values))\n",
    "  return new_df\n",
    "\n",
    "df_const = encode_categorical(df_const, cat_cols)\n",
    "df_mode = encode_categorical(df_mode, cat_cols)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.3.1 Uploading processed datasets\n",
    "\n",
    "Before getting to balancing the datasets we will upload them to the Seclea Plaform.\n",
    "\n",
    "- We define the metadata for the dataset - if there have been any changes since the original dataset we need to put that here, otherwise we can reuse the original metadata. In this case we have dropped some of the continuous feature columns so we will need to redefine\n",
    "\n",
    "- We define the transformations that took place between the last state we uploaded and this dataset. This is a list of functions and arguments. See docs.seclea.com for more details of the correct formatting."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define the metadata\n",
    "# NOTE even though we defined an index initially, because this dataset has been \n",
    "# augmented, the index has been dropped so now there is no specific index column.\n",
    "processed_metadata = {\n",
    "                  \"outcome_name\": \"fraud_reported\", \n",
    "                  \"continuous_features\": [\"total_claim_amount\",\n",
    "                                          'policy_annual_premium',\n",
    "                                          'capital-gains',\n",
    "                                          'capital-loss',\n",
    "                                          'injury_claim',\n",
    "                                          'property_claim',\n",
    "                                          'incident_hour_of_the_day',\n",
    "                                          ]}\n",
    "\n",
    "# here we need to define the transformations we applied to our original dataset\n",
    "# to get it to this point.\n",
    "# see the documentation for more details of the formatting this needs.\n",
    "\n",
    "const_processed_transformations = [\n",
    "    encode_nans,\n",
    "    (drop_nulls, [null_thresh]),\n",
    "    (fill_nan_const, [const_val]),\n",
    "    (drop_correlated, [correlation_threshold]),\n",
    "    (encode_categorical, [cat_cols]),\n",
    "]\n",
    "\n",
    "seclea.upload_dataset(dataset=df_const, \n",
    "                      dataset_name=\"Auto Insurance Fraud - Const Fill\", \n",
    "                      metadata=processed_metadata, \n",
    "                      parent=df, \n",
    "                      transformations=const_processed_transformations)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- We need to do this for the dataset that filled NaN values with the mode. We can reuse the metadata for the processed data as it is the same but we need to change the transformations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mode_processed_transformations = [\n",
    "    encode_nans,\n",
    "    (drop_nulls, [null_thresh]),\n",
    "    (fill_nan_mode, [nan_cols]),\n",
    "    (drop_correlated, [correlation_threshold]),\n",
    "    (encode_categorical, [cat_cols]),\n",
    "]\n",
    "\n",
    "seclea.upload_dataset(dataset=df_mode, \n",
    "                      dataset_name=\"Auto Insurance Fraud - Mode Fill\", \n",
    "                      metadata=processed_metadata, \n",
    "                      parent=df, \n",
    "                      transformations=mode_processed_transformations)\n",
    "\n",
    "def smote_balance(df):\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "\n",
    "    X1 = df.drop('fraud_reported', axis=1)\n",
    "    y1 = df.fraud_reported\n",
    "\n",
    "    sm = SMOTE(random_state=42)\n",
    "\n",
    "    X_sm, y_sm = sm.fit_resample(X1, y1)\n",
    "\n",
    "    print(f'''Shape of X before SMOTE: {X1.shape}\n",
    "    Shape of X after SMOTE: {X_sm.shape}''')\n",
    "    print(f'''Shape of y before SMOTE: {y1.shape}\n",
    "    Shape of y after SMOTE: {y_sm.shape}''')\n",
    "    return pd.concat([X_sm, y_sm], axis=1)\n",
    "\n",
    "# Using Smote to balance the dataset \n",
    "df_const_smote = smote_balance(df_const)\n",
    "df_mode_smote = smote_balance(df_mode)\n",
    "\n",
    "# here we need to define the transformations we applied to our original dataset\n",
    "smote_transformations = [\n",
    "    smote_balance,\n",
    "]\n",
    "\n",
    "seclea.upload_dataset(dataset=df_const_smote, \n",
    "                      dataset_name=\"Auto Insurance Fraud - Const fill - Smote\", \n",
    "                      metadata=processed_metadata, \n",
    "                      parent=df_const, \n",
    "                      transformations=smote_transformations)\n",
    "\n",
    "seclea.upload_dataset(dataset=df_mode_smote, \n",
    "                      dataset_name=\"Auto Insurance Fraud - Mode Fill - Smote\", \n",
    "                      metadata=processed_metadata, \n",
    "                      parent=df_mode, \n",
    "                      transformations=smote_transformations)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now head to platform.seclea.com again to take another look at the Datasets section. You will see that there is a lot more to look at this time.\n",
    "\n",
    "You can see here how the transformations are used to show you the history of the data and how it arrived in its final state."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.6 Building Train and Test Datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have finished processing our data, and logged it in the Platform, we will define a function to split the data for input to our training and evaluation code."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Splitting the dataset \n",
    "\n",
    "def get_test_train_splits(df, output_col, test_size, random_state):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    X = df.drop(output_col, axis=1)\n",
    "    y = df[output_col]\n",
    "\n",
    "    return train_test_split(X, y, test_size=test_size, stratify=y, random_state=random_state)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5.Modeling with Balancing Techniques"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we get started with the modelling. We will run the same models over each of our datasets to explore how the different processing of the data has affected our results.\n",
    "\n",
    "We will use three models from sklearn for this, DecisionTree, RandomForest and GradientBoosting Classifers. \n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from seclea_utils.get_model_manager import Frameworks\n",
    "\n",
    "classifiers = {\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
    "    \"GradientBoostingClassifier\": GradientBoostingClassifier()\n",
    "}\n",
    "\n",
    "datasets = [(\"Const Fill\", df_const), (\"Mode Fill\", df_mode), (\"Const Fill Smote\", df_const_smote), (\"Mode Fill Smote\", df_mode_smote)]\n",
    "\n",
    "for name, dataset in datasets:\n",
    "    X_train, X_test, y_train, y_test = get_test_train_splits(dataset, output_col=\"fraud_reported\", test_size=0.2, random_state=42)\n",
    "\n",
    "    for key, classifier in classifiers.items():\n",
    "        # cross validate to get an idea of generalisation.\n",
    "        training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n",
    "\n",
    "        # train on the full training set\n",
    "        classifier.fit(X_train, y_train)\n",
    "\n",
    "        # upload the fully trained model\n",
    "        seclea.upload_training_run(classifier, Frameworks.SKLEARN, dataset=dataset)\n",
    "\n",
    "        # test accuracy\n",
    "        y_preds = classifier.predict(X_test)\n",
    "        test_score = accuracy_score(y_test, y_preds)\n",
    "        print(f\"Classifier: {classifier.__class__.__name__} has a training score of {round(training_score.mean(), 3) * 100}% accuracy score on {name}\")\n",
    "        print(f\"Classifier: {classifier.__class__.__name__} has a test score of {round(test_score, 3) * 100}% accuracy score on {name}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analysis\n",
    "\n",
    "Head back to [platform.seclea.com](https://platform.seclea.com) and we can analyse our Models\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ]
}