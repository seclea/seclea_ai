{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Getting Started with Seclea!",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"https://github.com/seclea/seclea_ai/raw/dev/docs/media/logos/logo-light.png\" width=\"400\" alt=\"Seclea\" />"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Getting Started\n",
    "\n",
    "We will run through the basic process of using Seclea to record your data science work\n",
    "and explore the results in the Seclea Platform.\n",
    "\n",
    "For non data-scientists you will want to pay most attention to the Seclea Platform sections."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setting up\n",
    "\n",
    "Head to [platform.seclea.com](https://platform.seclea.com) and log in.\n",
    "\n",
    "Create a new project and give it a name and description.\n",
    "\n",
    "<img src=\"https://github.com/seclea/seclea_ai/media/notebooks/getting_started/create-new-project.png\" width=300/>\n",
    "<img src=\"https://github.com/seclea/seclea_ai/media/notebooks/getting_started/create-project-name-description.png\" width=300/>\n",
    "\n",
    "Go to project settings\n",
    "\n",
    "<img src=\"https://github.com/seclea/seclea_ai/media/notebooks/getting_started/project-settings.png\" width=300/>\n",
    "\n",
    "Select Compliance, Risk and Performance Templates for this project.\n",
    "These are optional but are needed to take advantage of Checks. If in doubt leave these empty for now and come back.\n",
    "\n",
    "TODO fill in images --- here ---"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Data\n",
    "\n",
    "[Download](https://raw.githubusercontent.com/mwitiderrick/insurancedata/master/insurance_claims.csv) the data for this tutorial"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install seclea-ai"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can upload the initial data to the Seclea Platform. This should include whatever information we know about the dataset at this point as metadata.\n",
    "\n",
    "There are only two keys to add in metadata for now - outcome_name and continuous_features.\n",
    "\n",
    "Here you will also have to log in to the Platform using the credentials given to you."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from seclea_ai import SecleaAI\n",
    "\n",
    "# load the data \n",
    "data = pd.read_csv('insurance_claims.csv', index=\"policy_number\")\n",
    "\n",
    "# upload the data in its initial state to the Seclea Platform\n",
    "# NOTE - use the organization name provided to you when issued credentials.\n",
    "seclea = SecleaAI(project_name=\"My Project\", organization='My Org')\n",
    "\n",
    "dataset_metadata = {\"outcome_name\": \"fraud_reported\", \n",
    "                    \"continuous_features\": [\n",
    "                                            \"total_claim_amount\",\n",
    "                                            'policy_annual_premium',\n",
    "                                            'capital-gains',\n",
    "                                            'capital-loss',\n",
    "                                            'injury_claim',\n",
    "                                            'property_claim',\n",
    "                                            'vehicle_claim',\n",
    "                                            'incident_hour_of_the_day',\n",
    "                                            ]}\n",
    "\n",
    "seclea.upload_dataset(dataset=data, dataset_name=\"Auto Insurance Fraud\", metadata=dataset_metadata)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transformations\n",
    "\n",
    "There is one important requirement when using Seclea to record your Data Science work, that is how\n",
    "to deal with transformations of the data.\n",
    "\n",
    "We require that all transformations are encapsulated in a function, that takes the data and returns the\n",
    "transformed data.\n",
    "\n",
    "In order to record and use the processing code effectively we need it to be packaged into functions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Creating a copy to isolate the original dataset\n",
    "df1 = data.copy(deep=True)\n",
    "\n",
    "def encode_nans(df):\n",
    "    # converting the special character to nans so we can use nan processing code\n",
    "    # available in pandas.\n",
    "    return df.replace('?', np.NaN)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df1 = encode_nans(df1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.4 Seclea Platform\n",
    "\n",
    "Now you should navigate in your browser to platform.seclea.com and login.\n",
    "\n",
    "You should see a dashboard etc. select the project\n",
    "\n",
    "Navigate to the Datasets section - under Prepare tab. See the preview and use the format check/PII check.\n",
    "\n",
    "Include some tasks for them to explore.\n",
    "\n",
    "Include screen shots."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4.Data preprocessing/Feature Engineering "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.1 Dealing with Missing values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We may think that None (or NaN) values are just zeroes because they represent the absence of a value. The main difference between zero and None value is that zero is a value (for example integer or float), while the None value represents the absence of that value.\n",
    "\n",
    "There are various techniques to replace missing value such as \n",
    "1. Fill NaN with Mean, Median or Mode of the data\n",
    "2. Fill NaN with a constant value\n",
    "3. Imputing with KNN\n",
    "4. Imputing with MICE(Multiple Imputation by Chained Equations)\n",
    "\n",
    "We will try two of these techniques, the constant fill for which we will use the value -1 as the fill value. We will also use the mode filling technique for a second version of the dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we define a function to carry out the dropping of columns that contain more than a certain proportion of nulls. We do this as these columns usually don't add useful information and only slow us down."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Drop the the column with certain proportion NaN value \n",
    "def drop_nulls(df, threshold):\n",
    "    cols = [x for x in df.columns if df[x].isnull().sum() / df.shape[0] > threshold]\n",
    "    return df.drop(columns=cols)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You will notice that we define the threshold value as a variable and pass it in. That is because we will be using this variable again later to upload these transformations. It is easier than copy and pasting the value."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# We choose 95% as our threshold\n",
    "null_thresh = 0.95\n",
    "df1 = drop_nulls(df1, threshold=null_thresh)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this case only _c39 will be dropped as it is 100% null values.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we will define a function that will replace nans with a constant value. Here we have shown an option that will deal directly with None values in the dataframe as well - not needed for this dataset but can be useful on others."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Changing 1st dataset with -1 \n",
    "\n",
    "def fill_nan_const(df, val):\n",
    "    \"\"\"Fill NaN values in the dataframe with a constant value\"\"\"\n",
    "    return df.replace(['None', np.nan], val) \n",
    " \n",
    "\n",
    "const_val = -1\n",
    "df_const = fill_nan_const(df1, const_val)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_const.isnull().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def fill_nan_mode(df, columns):\n",
    "    \"\"\"\n",
    "    Fills nans in specified columns with the mode of that column\n",
    "    Note that we want to make sure to not modify the dataset we passed in but to\n",
    "    return a new copy.\n",
    "    We do that by making a copy and specifying deep=True.\n",
    "    \"\"\"\n",
    "    new_df = df.copy(deep=True)\n",
    "    for col in df.columns:\n",
    "        if col in columns:\n",
    "            new_df[col] = df[col].fillna(df[col].mode()[0])\n",
    "    return new_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nan_cols = ['collision_type','property_damage','police_report_available']\n",
    "df_mode = fill_nan_mode(df1, nan_cols)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(df_mode)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_mode.isnull().sum()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def drop_correlated(data, thresh):\n",
    "    import numpy as np\n",
    "\n",
    "    # calculate correlations\n",
    "    corr_matrix = data.corr().abs()\n",
    "    # get the upper part of correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "    # columns with correlation above threshold\n",
    "    redundant = [column for column in upper.columns if any(upper[column] >= thresh)]\n",
    "    print(f\"Columns to drop with correlation > {thresh}: {redundant}\")\n",
    "    new_data = data.drop(columns=redundant)\n",
    "    return new_data\n",
    "\n",
    "correlation_threshold = 0.9\n",
    "\n",
    "df_const = drop_correlated(df_const, correlation_threshold)\n",
    "df_mode = drop_correlated(df_mode, correlation_threshold)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##4.3 Changing categorial data to numeric \n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we still have the same columns for both df_const and df_mode we are only looking for the columns in one dataset.\n",
    "\n",
    "If we had dropped different columns in each that would not be possible.\n",
    "\n",
    "Also an important note is that these columns are only considered categorical for encoding purposes! This doesn't mean that no other columns represent categorical values, just that if there are, they have already been encoded to numerical values in some way. This may be important for other datasets and analyses."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# find columns with categorical data for both dataset\n",
    "cat_cols = df_const.select_dtypes(include=['object']).columns.tolist()\n",
    "cat_cols"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we are using the sklearn LabelEncoder to encode each of the selected columns passed in. We do this to avoid encoding columns that already contain categorical data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def encode_categorical(df, cat_cols): \n",
    "  from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "  new_df = df.copy(deep=True)\n",
    "  for col in cat_cols:\n",
    "    if col in df.columns:\n",
    "        le = LabelEncoder()\n",
    "        le.fit(list(df[col].astype(str).values))\n",
    "        new_df[col] = le.transform(list(df[col].astype(str).values))\n",
    "  return new_df\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_const = encode_categorical(df_const, cat_cols)\n",
    "df_mode = encode_categorical(df_mode, cat_cols)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.3.1 Uploading processed datasets\n",
    "\n",
    "Before getting to balancing the datasets we will upload them to the Seclea Plaform.\n",
    "\n",
    "- We define the metadata for the dataset - if there have been any changes since the original dataset we need to put that here, otherwise we can reuse the original metadata. In this case we have dropped some of the continuous feature columns so we will need to redefine\n",
    "\n",
    "- We define the transformations that took place between the last state we uploaded and this dataset. This is a list of functions and arguments. See docs.seclea.com for more details of the correct formatting."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define the metadata\n",
    "# NOTE even though we defined an index initially, because this dataset has been \n",
    "# augmented, the index has been dropped so now there is no specific index column.\n",
    "processed_metadata = {\"index\": None, \n",
    "                  \"outcome_name\": \"fraud_reported\", \n",
    "                  \"continuous_features\": [\"total_claim_amount\",\n",
    "                                          'policy_annual_premium',\n",
    "                                          'capital-gains',\n",
    "                                          'capital-loss',\n",
    "                                          'injury_claim',\n",
    "                                          'property_claim',\n",
    "                                          'incident_hour_of_the_day',\n",
    "                                          ]}\n",
    "\n",
    "# here we need to define the transformations we applied to our original dataset\n",
    "# to get it to this point.\n",
    "# see the documentation for more details of the formatting this needs.\n",
    "\n",
    "const_processed_transformations = [\n",
    "    encode_nans,\n",
    "    (drop_nulls, [null_thresh]),\n",
    "    (fill_nan_const, [const_val]),\n",
    "    (drop_correlated, [correlation_threshold]),\n",
    "    (encode_categorical, [cat_cols]),\n",
    "]\n",
    "\n",
    "seclea.upload_dataset(dataset=df_const, \n",
    "                      dataset_name=\"Auto Insurance Fraud - Const Fill\", \n",
    "                      metadata=processed_metadata, \n",
    "                      parent=df, \n",
    "                      transformations=const_processed_transformations)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- We need to do this for the dataset that filled NaN values with the mode. We can reuse the metadata for the processed data as it is the same but we need to change the transformations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mode_processed_transformations = [\n",
    "    encode_nans,\n",
    "    (drop_nulls, [null_thresh]),\n",
    "    (fill_nan_mode, [nan_cols]),\n",
    "    (drop_correlated, [correlation_threshold]),\n",
    "    (encode_categorical, [cat_cols]),\n",
    "]\n",
    "\n",
    "seclea.upload_dataset(dataset=df_mode, \n",
    "                      dataset_name=\"Auto Insurance Fraud - Mode Fill\", \n",
    "                      metadata=processed_metadata, \n",
    "                      parent=df, \n",
    "                      transformations=mode_processed_transformations)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##4.4 Balancing the dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Checking for imbalance dataset (with half of the dataset)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(2.5,5)) \n",
    "plt.title(\"Fraud Transaction Distribution\") \n",
    "p1 = sns.countplot(df_const['fraud_reported'], palette = 'plasma') \n",
    "for p in p1.patches:\n",
    "    height = p.get_height()\n",
    "    p1.text(p.get_x()+p.get_width()/2.,\n",
    "            height,\n",
    "            f'{height/df.shape[0] * 100:.2f}%',\n",
    "            ha='center', fontsize=12)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Most machine learning models perform best with balanced datasets and tolerate imbalanced datasets to different levels. Imbalance very often causes poor predictions, especially for minority class samples. In this dataset we are dealing with a mildly imbalanced dataset however many fraud datasets can have fraud cases making up as little as 0.1% of samples and security datasets even fewer.\n",
    "\n",
    "We would expect to achieve fairly good accuracy on the basis of this, however we will explore the impact that balance can have on the accuracy of a model and see if balancing can be beneficial to us. \n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "There are more than 10 techniques available for balancing datasets. Out of all I have detailed the three most used tchniques\n",
    "\n",
    "1.   Random under sampling: Removing majority of the class and keeping the data same as minor class. The main drawback of this technique is it may remove the important information from the dataset  \n",
    "\n",
    "2.   Random oversampling techniques: adding more value to minority class when there is not enough data for minority class however this can cause overfitting and poor generalization. \n",
    "\n",
    "3. SMOTE (Synthetic Minority Oversampling Techniques ) the main idea of this techniuqe is it randomly picks a point from minority class and compute k-nearest neighbour for the point. Synthetic points are added between. \n",
    "\n",
    "In this dataset we will use SMOTE for balancind as random over sampling is prone to overfitting and undersampling will remove some information that may be useful to our model.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following function has some particular restrictions in order to work well with the Seclea Platform. We need the function that processes the dataset to return the dataset in a complete form - that is with both features and labels as part of the same DataFrame. \n",
    "\n",
    "This will likely be changed in the future but for now you will need to split the data in the function into features and labels for the oversampling and then concat them back together after sampling to return all together from the function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define a balancing function\n",
    "\n",
    "def smote_balance(df):\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "\n",
    "    X1 = df.drop('fraud_reported', axis=1)\n",
    "    y1 = df.fraud_reported\n",
    "\n",
    "    sm = SMOTE(random_state=42)\n",
    "\n",
    "    X_sm, y_sm = sm.fit_resample(X1, y1)\n",
    "\n",
    "    print(f'''Shape of X before SMOTE: {X1.shape}\n",
    "    Shape of X after SMOTE: {X_sm.shape}''')\n",
    "    print(f'''Shape of y before SMOTE: {y1.shape}\n",
    "    Shape of y after SMOTE: {y_sm.shape}''')\n",
    "    return pd.concat([X_sm, y_sm], axis=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Using Smote to balance the dataset \n",
    "df_const_smote = smote_balance(df_const)\n",
    "df_mode_smote = smote_balance(df_mode)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_const_smote.head(4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_mode_smote.head(4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.4.1 Upload Smote datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here again we need to upload the transformed datasets.\n",
    "It is easier here because we only have the one transformation to upload and the metadata we can reuse as this transformation didn't affect that.\n",
    "\n",
    "This is in many ways the easiest way to keep track of your datasets and transformations as it saves keeping track of too many functions or variables at any one time.\n",
    "\n",
    "Note that we include the processed datasets as the parent here, not the original dataset. That is because this dataset comes directly from the processed dataset with one transformation, not directly from the original data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# here we need to define the transformations we applied to our original dataset\n",
    "\n",
    "smote_transformations = [\n",
    "    smote_balance,\n",
    "]\n",
    "\n",
    "seclea.upload_dataset(dataset=df_const_smote, \n",
    "                      dataset_name=\"Auto Insurance Fraud - Const fill - Smote\", \n",
    "                      metadata=processed_metadata, \n",
    "                      parent=df_const, \n",
    "                      transformations=smote_transformations)\n",
    "\n",
    "seclea.upload_dataset(dataset=df_mode_smote, \n",
    "                      dataset_name=\"Auto Insurance Fraud - Mode Fill - Smote\", \n",
    "                      metadata=processed_metadata, \n",
    "                      parent=df_mode, \n",
    "                      transformations=smote_transformations)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now head to platform.seclea.com again to take another look at the Datasets section. You will see that there is a lot more to look at this time.\n",
    "\n",
    "You can see here how the transformations are used to show you the history of the data and how it arrived in its final state."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.6 Building Train and Test Datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have finished processing our data, and logged it in the Platform, we will define a function to split the data for input to our training and evaluation code."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Splitting the dataset \n",
    "\n",
    "def get_test_train_splits(df, output_col, test_size, random_state):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    X = df.drop(output_col, axis=1)\n",
    "    y = df[output_col]\n",
    "\n",
    "    return train_test_split(X, y, test_size=test_size, stratify=y, random_state=random_state)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5.Modeling with Balancing Techniques"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we get started with the modelling. We will run the same models over each of our datasets to explore how the different processing of the data has affected our results.\n",
    "\n",
    "We will use three models from sklearn for this, DecisionTree, RandomForest and GradientBoosting Classifers. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install tabulate\n",
    "from tabulate import tabulate\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we are defining our classifiers in a dictionary, this will help to simplify training code later on."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    " ### Modeling \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "classifiers = {\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
    "    \"GradientBoostingClassifier\": GradientBoostingClassifier()\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we are training all of our classifiers on each of the datasets in turn so that we can easily and rapidly evaluate their performance.\n",
    "\n",
    "Note the use of functions really simplifies our lives here, we do not have to repeat code here as in reality we are doing the same thing for each classifier and dataset so better to loop over each and save ourselves some typing!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from seclea_utils.get_model_manager import Frameworks\n",
    "\n",
    "datasets = [(\"Const Fill\", df_const), (\"Mode Fill\", df_mode), (\"Const Fill Smote\", df_const_smote), (\"Mode Fill Smote\", df_mode_smote)]\n",
    "\n",
    "for name, dataset in datasets:\n",
    "    X_train, X_test, y_train, y_test = get_test_train_splits(dataset, output_col=\"fraud_reported\", test_size=0.2, random_state=42)\n",
    "\n",
    "    for key, classifier in classifiers.items():\n",
    "        # cross validate to get an idea of generalisation.\n",
    "        training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n",
    "        # train on the full training set\n",
    "        classifier.fit(X_train, y_train)\n",
    "        # upload the fully trained model\n",
    "        seclea.upload_training_run(classifier, Frameworks.SKLEARN, dataset=dataset)\n",
    "        # test accuracy\n",
    "        y_preds = classifier.predict(X_test)\n",
    "        test_score = accuracy_score(y_test, y_preds)\n",
    "        print(f\"Classifier: {classifier.__class__.__name__} has a training score of {round(training_score.mean(), 3) * 100}% accuracy score on {name}\")\n",
    "        print(f\"Classifier: {classifier.__class__.__name__} has a test score of {round(test_score, 3) * 100}% accuracy score on {name}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So now we can see the overall results but here is a perfect opportunity to head to the Platform to dig deeper into our results and the performance differences.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ]
}